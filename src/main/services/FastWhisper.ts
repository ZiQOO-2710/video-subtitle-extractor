import { spawn } from 'child_process';
import * as path from 'path';
import * as fs from 'fs';

export interface FastWhisperOptions {
  language?: string;
  model?: 'tiny' | 'base' | 'small' | 'medium' | 'large';
  device?: 'cuda' | 'cpu';
}

export class FastWhisper {
  private mainWindow?: Electron.BrowserWindow;

  constructor(mainWindow?: Electron.BrowserWindow) {
    this.mainWindow = mainWindow;
  }

  /**
   * CUDA ê°€ì† ìŒì„± ì¸ì‹ (faster-whisper ì‚¬ìš©)
   */
  async processAudio(audioPath: string, options: FastWhisperOptions = {}): Promise<any> {
    const {
      language = 'ja',
      model = 'base',
      device = 'cuda'  // GTX 980Ti CUDA ì‚¬ìš©
    } = options;

    return new Promise((resolve, reject) => {
      console.log(`ğŸš€ CUDA ê°€ì† ìŒì„± ì¸ì‹ ì‹œì‘: ${audioPath}`);

      // Python script ì‹¤í–‰
      const pythonScript = `
import sys
from faster_whisper import WhisperModel
import json

# CUDA ëª¨ë¸ ë¡œë“œ (GTX 980Ti ìµœì í™” - Maxwell ì•„í‚¤í…ì²˜ìš© float32)
model = WhisperModel("${model}", device="${device}", compute_type="float32")

# ìŒì„± ì¸ì‹ ì‹¤í–‰
segments, info = model.transcribe("${audioPath.replace(/\\/g, '\\\\')}", language="${language}")

# SRT í˜•ì‹ìœ¼ë¡œ ë³€í™˜
result = []
for segment in segments:
    result.append({
        "start": segment.start,
        "end": segment.end,
        "text": segment.text
    })

# ê²°ê³¼ ì¶œë ¥
print(json.dumps({
    "segments": result,
    "language": info.language,
    "duration": info.duration
}))
`;

      const tempScriptPath = path.join(process.cwd(), 'temp', 'whisper_cuda.py');
      
      // ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
      const tempDir = path.dirname(tempScriptPath);
      if (!fs.existsSync(tempDir)) {
        fs.mkdirSync(tempDir, { recursive: true });
      }

      // Python ìŠ¤í¬ë¦½íŠ¸ ì €ì¥
      fs.writeFileSync(tempScriptPath, pythonScript);

      const pythonProcess = spawn('python', [tempScriptPath]);
      let stdout = '';
      let stderr = '';

      pythonProcess.stdout.on('data', (data) => {
        const output = data.toString();
        stdout += output;
        
        // GPU ì‚¬ìš©ë¥  ì²´í¬ ë° ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
        if (this.mainWindow) {
          this.mainWindow.webContents.send('progress-update', {
            stage: 'speech-recognition',
            progress: 50, // GPU ì²˜ë¦¬ëŠ” ë§¤ìš° ë¹ ë¥´ë¯€ë¡œ ëŒ€ëµì ì¸ ì§„í–‰ë¥ 
            message: 'ğŸš€ GPU ê°€ì† ìŒì„± ì¸ì‹ ì¤‘...'
          });
        }
      });

      pythonProcess.stderr.on('data', (data) => {
        const error = data.toString();
        stderr += error;
        console.log('CUDA Whisper stderr:', error);
      });

      pythonProcess.on('close', (code) => {
        // ì„ì‹œ íŒŒì¼ ì •ë¦¬
        try {
          fs.unlinkSync(tempScriptPath);
        } catch (err) {
          console.warn('ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨:', err);
        }

        if (code === 0) {
          try {
            const result = JSON.parse(stdout.trim());
            console.log('ğŸ‰ CUDA ê°€ì† ì²˜ë¦¬ ì™„ë£Œ!');
            resolve(result);
          } catch (error) {
            reject(new Error(`ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨: ${error}`));
          }
        } else {
          reject(new Error(`CUDA Whisper ì‹¤í–‰ ì‹¤íŒ¨ (ì½”ë“œ ${code}): ${stderr}`));
        }
      });

      pythonProcess.on('error', (error) => {
        console.error('CUDA Whisper í”„ë¡œì„¸ìŠ¤ ì˜¤ë¥˜:', error);
        reject(error);
      });
    });
  }

  /**
   * CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ (ì‹¤ì œ ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸)
   */
  async checkCudaAvailable(): Promise<boolean> {
    return new Promise((resolve) => {
      const checkScript = `
import torch
from faster_whisper import WhisperModel
import sys

try:
    print('CUDA_AVAILABLE:' + str(torch.cuda.is_available()))
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        print('GPU_NAME:' + gpu_name)
        
        # ì‹¤ì œ ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸ (tiny ëª¨ë¸ë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)
        print('TESTING_MODEL_LOAD')
        model = WhisperModel("tiny", device="cuda", compute_type="float32")
        print('MODEL_LOAD_SUCCESS:True')
        del model
        torch.cuda.empty_cache()
    else:
        print('MODEL_LOAD_SUCCESS:False')
except Exception as e:
    print(f'MODEL_LOAD_ERROR:{str(e)}')
    print('MODEL_LOAD_SUCCESS:False')
`;

      const tempPath = path.join(process.cwd(), 'temp', 'cuda_check.py');
      
      // ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
      const tempDir = path.dirname(tempPath);
      if (!fs.existsSync(tempDir)) {
        fs.mkdirSync(tempDir, { recursive: true });
      }
      
      fs.writeFileSync(tempPath, checkScript);

      const checkProcess = spawn('python', [tempPath]);
      let output = '';
      let errorOutput = '';

      checkProcess.stdout.on('data', (data) => {
        output += data.toString();
      });

      checkProcess.stderr.on('data', (data) => {
        errorOutput += data.toString();
      });

      checkProcess.on('close', (code) => {
        try {
          fs.unlinkSync(tempPath);
        } catch (err) {
          // ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ ë¬´ì‹œ
        }
        
        const cudaAvailable = output.includes('CUDA_AVAILABLE:True');
        const modelLoadSuccess = output.includes('MODEL_LOAD_SUCCESS:True');
        const gpuName = output.match(/GPU_NAME:(.+)/)?.[1];
        
        if (cudaAvailable && modelLoadSuccess && gpuName) {
          console.log(`ğŸš€ CUDA ê°€ì† ì‚¬ìš© ê°€ëŠ¥: ${gpuName} (ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸ í†µê³¼)`);
          resolve(true);
        } else {
          if (cudaAvailable && !modelLoadSuccess) {
            console.log(`âš ï¸ CUDAëŠ” ê°ì§€ë˜ë‚˜ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: ${gpuName}`);
            if (output.includes('MODEL_LOAD_ERROR:')) {
              const errorMatch = output.match(/MODEL_LOAD_ERROR:(.+)/);
              if (errorMatch) {
                console.log(`CUDA ì—ëŸ¬: ${errorMatch[1]}`);
              }
            }
          }
          resolve(false);
        }
      });

      checkProcess.on('error', () => {
        resolve(false);
      });
    });
  }
}