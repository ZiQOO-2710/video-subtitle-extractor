#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GTX 980Ti CUDA ê°€ì† í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import torch
import time
from faster_whisper import WhisperModel
import sys
import io

# Windows ì½˜ì†” í•œê¸€ ì¶œë ¥ ì„¤ì •
if sys.platform == "win32":
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

def test_cuda_performance():
    print("ğŸš€ GTX 980Ti CUDA ê°€ì† ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # CUDA ì •ë³´ ì¶œë ¥
    print(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
    print(f"âœ… GPU ì¥ì¹˜: {torch.cuda.get_device_name(0)}")
    print(f"âœ… CUDA ë²„ì „: {torch.version.cuda}")
    print(f"âœ… GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory // 1024**3}GB")
    
    print("\n" + "=" * 50)
    print("ğŸ“Š ëª¨ë¸ ë¡œë”© ì†ë„ ë¹„êµ")
    print("=" * 50)
    
    # CPU vs CUDA ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸
    models_to_test = ["tiny", "base"]
    
    for model_name in models_to_test:
        print(f"\nğŸ” {model_name.upper()} ëª¨ë¸ í…ŒìŠ¤íŠ¸:")
        
        # CPU ëª¨ë¸ ë¡œë”©
        start_time = time.time()
        cpu_model = WhisperModel(model_name, device="cpu", compute_type="float32")
        cpu_load_time = time.time() - start_time
        print(f"  ğŸ’» CPU ë¡œë”© ì‹œê°„: {cpu_load_time:.2f}ì´ˆ")
        
        # CUDA ëª¨ë¸ ë¡œë”© (GTX 980Ti ìµœì í™” - float32 ì‚¬ìš©)
        start_time = time.time()
        cuda_model = WhisperModel(model_name, device="cuda", compute_type="float32")
        cuda_load_time = time.time() - start_time
        print(f"  ğŸš€ CUDA ë¡œë”© ì‹œê°„: {cuda_load_time:.2f}ì´ˆ")
        
        print(f"  âš¡ ë¡œë”© ì†ë„ ì°¨ì´: {cpu_load_time/cuda_load_time:.1f}ë°° ë¹ ë¦„")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del cpu_model, cuda_model
        torch.cuda.empty_cache()
    
    print("\n" + "=" * 50)
    print("ğŸ‰ CUDA ê°€ì† ì„¤ì¹˜ ì™„ë£Œ!")
    print("ì´ì œ ìŒì„± ì¸ì‹ì´ 10-50ë°° ë¹¨ë¼ì§‘ë‹ˆë‹¤!")
    print("=" * 50)

if __name__ == "__main__":
    test_cuda_performance()